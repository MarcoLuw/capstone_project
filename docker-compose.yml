version: '3.8'
services:
  db:
    image: postgres:16-alpine
    container_name: postgres
    deploy:
      resources:
        limits:
          cpus: '0.5'
    ports:
      - '5432:5432'
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d metastore_db"]
      interval: 10s
      timeout: 10s
      retries: 10
    volumes:
      - ./data/db:/var/lib/postgresql/data/
      - ./init_db.sh:/docker-entrypoint-initdb.d/init_db.sh
    entrypoint: ["/bin/sh", "-c", "chmod +x /docker-entrypoint-initdb.d/init_db.sh && docker-entrypoint.sh postgres && /docker-entrypoint-initdb.d/init_db.sh"]
    restart: always
    networks:
      - backendnetwork

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        TARGETPLATFORM: linux/amd64
    container_name: django-backend
    deploy:
      resources:
        limits:
          cpus: '1'
    ports:
      - 8000:8000
    restart: always
    networks:
      - backendnetwork
    volumes:
      - './backend:/app/backend'
      - '/var/run/docker.sock:/var/run/docker.sock'
    depends_on:
      - db
  
  # Data service
  minio:
    image: minio/minio:RELEASE.2024-03-21T23-13-43Z-cpuv1
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data_service/minio-data/data-1:/data-1
      - ./data_service/minio-data/data-2:/data-2
      - ./data_service/minio-data/data-3:/data-3
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-f",
          "http://localhost:9000/minio/health/live"
        ]
      interval: 30s
      timeout: 20s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: miniopassword
      MINIO_VOLUMES: "/data-{1...3}"
      MINIO_OPTS: "--console-address :9001"
      MINIO_CONFIG_ENV_FILE: "/etc/config.env"
    command: server --console-address ":9001" /data
    networks:
      - backendnetwork
  mc:
    container_name: minio-client
    depends_on:
      - minio
    image: minio/mc
    deploy:
      resources:
        limits:
          cpus: "0.5"
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=miniopassword
      - AWS_REGION=us-east-1
    volumes:
      - ./data_service/policy.json:/policy.json
    entrypoint: >
      /bin/sh -c " until (/usr/bin/mc config host add minio http://minio:9000 minioadmin miniopassword) do echo '...waiting...' && sleep 1; done; 
      /usr/bin/mc admin policy create minio user_policy policy.json;
      /usr/bin/mc mb minio/hive/warehouse;
      /usr/bin/mc mb minio/logging/spark-events;
      tail -f /dev/null "
    networks:
      - backendnetwork

  hive-metastore:
    hostname: hive-metastore
    container_name: hive-metastore
    build: ./data_service/hive-config
    deploy:
      resources:
        limits:
          cpus: "0.5"
    ports:
      - 9083:9083
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
    volumes:
      - ./data_service/hive-config:/opt/hive/conf
      - ./data_service/hadoop-libs/postgresql-42.7.2.jar:/opt/hive/lib/postgresql-42.7.2.jar
      - ./data_service/hadoop-libs/hadoop-aws-3.3.4.jar:/opt/hive/lib/hadoop-aws-3.3.4.jar
      - ./data_service/hadoop-libs/aws-java-sdk-bundle-1.12.541.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.541.jar
    depends_on:
      db:
        condition: service_healthy
    networks:
      - backendnetwork

  spark-master:
    hostname: spark-master
    container_name: spark-master
    image: cluster-apache-spark:3.4.2
    deploy:
      resources:
        limits:
          cpus: "0.5"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    ports:
      - "8080:8080"
      - "7077:7077"
      - "8889:8889"
      - "6066:6066"
    volumes:
      - ./data_service/spark_apps/:/opt/spark_apps/
      - ./data_service/tests/:/opt/tests/
      - ./data_service/hadoop-libs/aws-java-sdk-1.12.541.jar:/opt/spark/jars/aws-java-sdk-1.12.541.jar
      - ./data_service/hadoop-libs/aws-java-sdk-bundle-1.12.541.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.541.jar
      - ./data_service/hadoop-libs/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./data_service/spark-config/conf/master/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./data_service/hadoop-libs/delta-core_2.12-2.4.0.jar:/opt/spark/jars/delta-core_2.12-2.4.0.jar
      - ./data_service/hadoop-libs/delta-storage-2.4.0.jar:/opt/spark/jars/delta-storage-2.4.0.jar
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
      - PYSPARK_DRIVER_PYTHON=python3
      # - PYSPARK_DRIVER_PYTHON_OPTS=jupyter notebook --no-browser --port=8889 --allow-root --ip=0.0.0.0
    networks:
      - backendnetwork

  spark-worker:
    image: cluster-apache-spark:3.4.2
    ports:
      - "8081-8099:8080"
      - "7078-7099:7000"
    depends_on:
      - spark-master
    deploy:
      resources:
        limits:
          cpus: "0.5"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
    volumes:
      - ./data_service/spark_apps:/opt/spark_apps
      - ./data_service/spark-config/conf/worker/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./data_service/hadoop-libs/aws-java-sdk-1.12.541.jar:/opt/spark/jars/aws-java-sdk-1.12.541.jar
      - ./data_service/hadoop-libs/aws-java-sdk-bundle-1.12.541.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.541.jar
      - ./data_service/hadoop-libs/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./data_service/hadoop-libs/delta-core_2.12-2.4.0.jar:/opt/spark/jars/delta-core_2.12-2.4.0.jar
      - ./data_service/hadoop-libs/delta-storage-2.4.0.jar:/opt/spark/jars/delta-storage-2.4.0.jar
    networks:
      - backendnetwork

  trino:
    hostname: trino
    container_name: trino
    image: trinodb/trino:442 
    deploy:
      resources:
        limits:
          cpus: "0.5"
    ports: 
      - 8888:8888
    volumes:
      - ./data_service/trino-config/etc/catalog:/etc/trino/catalog
      - ./data_service/trino-config/etc/config.properties:/etc/trino/config.properties
      - ./data_service/trino-config/etc/node.properties:/etc/trino/node.properties
      - ./data_service/trino-config/coordinator_config.properties:/etc/trino/config.properties
    networks:
      - backendnetwork

  trino-worker:
    hostname: trino-worker
    container_name: trino-worker
    image: trinodb/trino:442
    deploy:
      resources:
        limits:
          cpus: "0.5"
    volumes:
      - ./data_service/trino-config/etc/catalog:/etc/trino/catalog
      - ./data_service/trino-config/etc/config.properties:/etc/trino/config.properties
      - ./data_service/trino-config/etc/node.properties:/etc/trino/node.properties
      - ./data_service/trino-config/worker_config.properties:/opt/trino/config.properties
    networks:
      - backendnetwork

networks:
  backendnetwork:
    driver: bridge