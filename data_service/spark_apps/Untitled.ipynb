{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5961e2bd-9e6d-4ad2-b34e-41ac7592b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8d62f2f-bd15-4060-87e2-959a4c81de47;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 1998ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8d62f2f-bd15-4060-87e2-959a4c81de47\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/90ms)\n",
      "24/06/03 05:39:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from minio import Minio\n",
    "from utils.common import get_spark_session, get_minio_client\n",
    "from utils.bronze import Bronze\n",
    "from utils.silver import Silver\n",
    "from utils.gold import Gold\n",
    "\n",
    "username = 'camonco'\n",
    "\n",
    "spark = get_spark_session(\n",
    "        f\"{username}_ETL\",\n",
    "        username,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb00e745-9bf6-4c63-b37b-49c7f0429865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = 'shopee'\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_camonco.shopee_fact_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675e0831-ec77-4e63-91bf-53a69cc70988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 05:44:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/06/03 05:47:00 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`gold_camonco`.`shopee_fact_sales` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/06/03 05:47:01 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/06/03 05:53:09 WARN S3AInstrumentation: Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics{counters=((multipart_upload_completed.failures=0) (action_executor_acquired.failures=0) (action_executor_acquired=0) (stream_write_total_time=0) (op_hflush=0) (object_multipart_aborted.failures=0) (op_abort=0) (stream_write_total_data=4745) (stream_write_block_uploads=1) (object_multipart_aborted=0) (stream_write_exceptions=0) (op_abort.failures=0) (op_hsync=0) (stream_write_bytes=4745) (multipart_upload_completed=0) (stream_write_queue_duration=0) (stream_write_exceptions_completing_upload=0));\n",
      "gauges=((stream_write_block_uploads_pending=1) (stream_write_block_uploads_data_pending=0));\n",
      "minimums=((action_executor_acquired.min=-1) (object_multipart_aborted.min=-1) (op_abort.failures.min=-1) (multipart_upload_completed.failures.min=-1) (multipart_upload_completed.min=-1) (object_multipart_aborted.failures.min=-1) (action_executor_acquired.failures.min=-1) (op_abort.min=-1));\n",
      "maximums=((action_executor_acquired.failures.max=-1) (op_abort.failures.max=-1) (op_abort.max=-1) (object_multipart_aborted.failures.max=-1) (multipart_upload_completed.max=-1) (action_executor_acquired.max=-1) (object_multipart_aborted.max=-1) (multipart_upload_completed.failures.max=-1));\n",
      "means=((action_executor_acquired.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.failures.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.failures.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.mean=(samples=0, sum=0, mean=0.0000)) (action_executor_acquired.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.mean=(samples=0, sum=0, mean=0.0000)));\n",
      ", blocksActive=0, blockUploadsCompleted=0, blocksAllocated=1, blocksReleased=1, blocksActivelyAllocated=0, transferDuration=0 ms, totalUploadDuration=0 ms, effectiveBandwidth=0.0 bytes/s}\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = f\"\"\"\n",
    "                SELECT\n",
    "                    monotonically_increasing_id() AS sale_key,\n",
    "                    date_dim.date_key AS date_key,\n",
    "                    product_dim.product_key AS product_key,\n",
    "                    customer_dim.customer_key AS customer_key,\n",
    "                    promotion_dim.promotion_key AS promotion_key,\n",
    "                    shipment_dim.shipment_key AS shipment_key,\n",
    "                    silver.order_number,\n",
    "                    silver.order_quantity,\n",
    "                    silver.unit_price,\n",
    "                    silver.unit_discount,\n",
    "                    silver.sales_amount,\n",
    "                    CAST(silver.order_date AS DATE) AS order_date,\n",
    "                    silver.ship_date,\n",
    "                    silver.payment_date\n",
    "                FROM\n",
    "                    silver_{username}.{source}_sales_silver AS silver\n",
    "                LEFT JOIN\n",
    "                    gold_{username}.dim_date AS date_dim\n",
    "                ON\n",
    "                    CAST(date_format(silver.order_date, 'yyyyMMdd') AS INT) = date_dim.date_key\n",
    "                LEFT JOIN\n",
    "                    gold_{username}.dim_product AS product_dim\n",
    "                ON\n",
    "                    silver.product_key = product_dim.product_key\n",
    "                LEFT JOIN\n",
    "                    gold_{username}.dim_customer AS customer_dim\n",
    "                ON\n",
    "                    silver.customer_name = customer_dim.customer_name\n",
    "                LEFT JOIN\n",
    "                    gold_{username}.dim_promotion AS promotion_dim\n",
    "                ON\n",
    "                    silver.promotion = promotion_dim.promotion\n",
    "                LEFT JOIN\n",
    "                    gold_{username}.dim_shipment AS shipment_dim\n",
    "                ON silver.shipping_company = shipment_dim.shipping_company\n",
    "            \"\"\"\n",
    "\n",
    "fact_sales_df = spark.sql(query)\n",
    "\n",
    "            # Save fact_sales DataFrame to Delta Lake with partitioning and Z-Ordering\n",
    "fact_sales_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .partitionBy(\"date_key\") \\\n",
    "                .save(f\"s3a://{username}/gold/shopee_fact_sales\")\n",
    "\n",
    "            # Create Delta table for easy querying\n",
    "spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS gold_{username}.shopee_fact_sales\n",
    "                USING DELTA \n",
    "                LOCATION 's3a://{username}/gold/shopee_fact_sales'\n",
    "            \"\"\")\n",
    "\n",
    "            # Optimize and Z-Order the table\n",
    "spark.sql(f\"\"\"\n",
    "                OPTIMIZE gold_{username}.shopee_fact_sales\n",
    "                ZORDER BY (product_key, customer_key)\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c0f4da-c486-431a-848b-2251940a5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_key: long (nullable = true)\n",
      " |-- date_key: integer (nullable = true)\n",
      " |-- product_key: string (nullable = true)\n",
      " |-- customer_key: long (nullable = true)\n",
      " |-- promotion_key: long (nullable = true)\n",
      " |-- shipment_key: long (nullable = true)\n",
      " |-- order_number: string (nullable = true)\n",
      " |-- order_quantity: integer (nullable = true)\n",
      " |-- unit_price: float (nullable = true)\n",
      " |-- unit_discount: float (nullable = true)\n",
      " |-- sales_amount: double (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- ship_date: date (nullable = true)\n",
      " |-- payment_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from gold_camonco.shopee_fact_sales limit 10\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e412768d-db13-45b4-94fe-71ac5b9f0af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS bronze_kayden CASCADE\")\n",
    "spark.sql(\"DROP DATABASE IF EXISTS silver_kayden CASCADE\")\n",
    "spark.sql(\"DROP DATABASE IF EXISTS gold_kayden CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df14bbb-ef1e-47be-ac6b-a970ae55ae0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `gold_kayden`.`shopee_fact_sales` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [gold_kayden, shopee_fact_sales], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kayden_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM gold_kayden.shopee_fact_sales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m kayden_df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `gold_kayden`.`shopee_fact_sales` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [gold_kayden, shopee_fact_sales], [], false\n"
     ]
    }
   ],
   "source": [
    "kayden_df = spark.sql(\"SELECT * FROM gold_kayden.shopee_fact_sales\")\n",
    "\n",
    "kayden_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f21c8a1-6b8d-4e07-a1f9-b4fa0fa5438d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[namespace: string, tableName: string, isTemporary: boolean]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN gold_kayden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de34f7-9927-4495-bb79-49283f626d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
