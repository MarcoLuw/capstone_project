version: '3.7'

services:
  minio:
    image: minio/minio:RELEASE.2024-03-21T23-13-43Z-cpuv1
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./minio-data/data-1:/data-1
      - ./minio-data/data-2:/data-2
      - ./minio-data/data-3:/data-3
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-f",
          "http://localhost:9000/minio/health/live"
        ]
      interval: 30s
      timeout: 20s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: miniopassword
      MINIO_VOLUMES: "/data-{1...3}"
      MINIO_OPTS: "--console-address :9001"
      MINIO_CONFIG_ENV_FILE: "/etc/config.env"
    command: server --console-address ":9001" /data
    networks:
      - backend_backendnetwork
  mc:
    container_name: minio-client
    depends_on:
      - minio
    image: minio/mc
    deploy:
      resources:
        limits:
          cpus: "0.5"
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=miniopassword
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c " until (/usr/bin/mc config host add minio http://minio:9000 minioadmin miniopassword) do echo '...waiting...' && sleep 1; done; 
      /usr/bin/mc mb minio/hive;
      /usr/bin/mc md minio/logging/spark-events;
      /usr/bin/mc md minio/data_services;
      tail -f /dev/null "
    networks:
      - backend_backendnetwork

  postgres:
    hostname: postgres
    container_name: postgres
    image: postgres:16-alpine
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: metastore_db
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d metastore_db"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - backend_backendnetwork

  hive-metastore:
    hostname: hive-metastore
    container_name: hive-metastore
    build: ./hive-config
    ports:
      - 9083:9083
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
    deploy:
      resources:
        limits:
          cpus: "0.5"
    volumes:
      - ./hive-config:/opt/hive/conf
      - ./hadoop-libs/postgresql-42.7.2.jar:/opt/hive/lib/postgresql-42.7.2.jar
      - ./hadoop-libs/hadoop-aws-3.3.4.jar:/opt/hive/lib/hadoop-aws-3.3.4.jar
      - ./hadoop-libs/aws-java-sdk-bundle-1.12.541.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.541.jar
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - backend_backendnetwork

  spark-master:
    hostname: spark-master
    container_name: spark-master
    image: cluster-apache-spark:3.4.2
    deploy:
      resources:
        limits:
          cpus: "0.5"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark-apps/:/opt/spark-apps/
      - ./hadoop-libs/aws-java-sdk-1.12.541.jar:/opt/spark/jars/aws-java-sdk-1.12.541.jar
      - ./hadoop-libs/aws-java-sdk-bundle-1.12.541.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.541.jar
      - ./hadoop-libs/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./spark-config/conf/master/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./spark-config/conf/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./hadoop-libs/delta-core_2.13-2.4.0.jar:/opt/spark/jars/delta-core_2.13-2.4.0.jar
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
    networks:
      - backend_backendnetwork

  spark-worker:
    image: cluster-apache-spark:3.4.2
    ports:
      - "8081-8099:8080"
      - "7078-7099:7000"
    depends_on:
      - spark-master
    deploy:
      resources:
        limits:
          cpus: "0.5"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-config/conf/worker/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./spark-config/conf/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./hadoop-libs/aws-java-sdk-1.12.541.jar:/opt/spark/jars/aws-java-sdk-1.12.541.jar
      - ./hadoop-libs/aws-java-sdk-bundle-1.12.541.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.541.jar
      - ./hadoop-libs/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./hadoop-libs/delta-core_2.13-2.4.0.jar:/opt/spark/jars/delta-core_2.13-2.4.0.jar
    networks:
      - backend_backendnetwork

  spark-history-server:
    container_name: spark-history
    image: cluster-apache-spark:3.4.2
    depends_on:
      - spark-master
    deploy:
      resources:
        limits:
          cpus: "0.5"
    volumes:
      - ./spark-config/conf/history/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - '18080:18080'
    networks:
      - data-services-network

  # kyuubi:
  #   hostname: kyuubi
  #   container_name: kyuubi
  #   image: apache/kyuubi:1.8.0-spark
  #   volumes:
  #     - ./kyuubi-config/kyuubi-defaults.conf:/opt/kyuubi/conf/kyuubi-defaults.conf
  #   ports:
  #     - "10009:10009"
  #     - "10099:10099"
  #   depends_on:
  #     - spark-master
  #   networks:
      # - data-services-network
      
networks:
  backend_backendnetwork:
    external: true
